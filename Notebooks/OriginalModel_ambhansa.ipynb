{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the original pretrained LPRNet Model"
      ],
      "metadata": {
        "id": "Dr0RMzp7P1NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRyBl-taemk1",
        "outputId": "4b3f20fc-25be-4b9e-c2ac-20afb33bba67"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/16.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/16.0 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/16.0 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m14.8/16.0 MB\u001b[0m \u001b[31m211.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m209.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuvN1H7VbMD-",
        "outputId": "2d6cb7ff-608a-4e95-a20e-edf0a547225f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'LPRNet_Pytorch' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sirius-ai/LPRNet_Pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LPRNet_Pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sT6ZsOhcQzE",
        "outputId": "ac9d0277-41f5-48ae-80ef-b53fc929bd28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LPRNet_Pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speed and Accuracy"
      ],
      "metadata": {
        "id": "Z1YOnsnPP92I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### command to test the pretrained model"
      ],
      "metadata": {
        "id": "n0tugjRpY1a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python test_LPRNet.py --test_img_dirs 'data/test' --pretrained_model 'weights/Final_LPRNet_model.pth'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUE8q2zVdMHT",
        "outputId": "2aefa7bb-3a08-4bd2-ae14-69600654d93c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lprnet.load_state_dict(torch.load(args.pretrained_model))\n",
            "load pretrained model successful!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.901 [901:60:39:1000]\n",
            "[Info] Test Speed: 0.2322s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model.LPRNet import build_lprnet\n",
        "\n",
        "# Load pretrained model\n",
        "model = build_lprnet(lpr_max_len=8, phase=False, class_num=68, dropout_rate=0.5)\n",
        "model.load_state_dict(torch.load('./weights/Final_LPRNet_model.pth', map_location = torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# Export to ONNX\n",
        "dummy_input = torch.randn(1, 3, 24, 94)  # Input size for LPRNet\n",
        "torch.onnx.export(model, dummy_input, \"./weights/Original_LPRNet.onnx\", verbose=True)\n",
        "print(\"Model exported to ONNX format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSHRIfjLd6h6",
        "outputId": "6a12aa05-60e0-4a2b-d973-8b57d98b7b34"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-0f599f939cf6>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./weights/Final_LPRNet_model.pth', map_location = torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to ONNX format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model.LPRNet import build_lprnet\n",
        "\n",
        "# Load pretrained model\n",
        "model = build_lprnet(lpr_max_len=8, phase=False, class_num=68, dropout_rate=0.5)\n",
        "model.load_state_dict(torch.load('./weights/Final_LPRNet_model.pth', map_location = torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# Export to ONNX\n",
        "dummy_input = torch.randn(1, 3, 24, 94)  # Input size for LPRNet\n",
        "torch.onnx.export(model, dummy_input, \"./weights/Original_LPRNet.onnx\", verbose=True)\n",
        "print(\"Model exported to ONNX format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv_R4NX1eOnn",
        "outputId": "57efc279-db4c-4ae4-e7b6-a7fa8b946d76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-0f599f939cf6>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./weights/Final_LPRNet_model.pth', map_location = torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to ONNX format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Size"
      ],
      "metadata": {
        "id": "bex6sBFiQBzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check ONNX file size\n",
        "import os\n",
        "onnx_file = \"./weights/Original_LPRNet.onnx\"\n",
        "size_mb = os.path.getsize(onnx_file) / (1024 * 1024)\n",
        "print(f\"ONNX Model Size: {size_mb:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-BrRNdmeXlW",
        "outputId": "87d1e7c3-b4e6-49fd-f3f9-763314e8fd4f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Model Size: 1.91 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non Zero Parameters Size"
      ],
      "metadata": {
        "id": "4r0AgUuVQFoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import numpy as np\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_file = \"./weights/Original_LPRNet.onnx\"\n",
        "onnx_model = onnx.load(onnx_file)\n",
        "\n",
        "# Data type size lookup table (in bytes)\n",
        "dtype_size_map = {\n",
        "    onnx.TensorProto.FLOAT: 4,    # 32-bit float\n",
        "    onnx.TensorProto.INT64: 8,   # 64-bit integer\n",
        "    onnx.TensorProto.INT32: 4,   # 32-bit integer\n",
        "    onnx.TensorProto.INT16: 2,   # 16-bit integer\n",
        "    onnx.TensorProto.INT8: 1,    # 8-bit integer\n",
        "    onnx.TensorProto.UINT8: 1    # 8-bit unsigned integer\n",
        "}\n",
        "\n",
        "# Calculate non-zero size\n",
        "non_zero_size_bytes = 0\n",
        "for initializer in onnx_model.graph.initializer:\n",
        "    dtype = initializer.data_type\n",
        "    dtype_size = dtype_size_map.get(dtype, 0)  # Get the size of each data type\n",
        "    weights = np.frombuffer(initializer.raw_data, dtype=np.float32)  # Assume float32 by default\n",
        "    non_zero_count = np.count_nonzero(weights)\n",
        "    non_zero_size_bytes += non_zero_count * dtype_size\n",
        "\n",
        "# Convert size to MB\n",
        "non_zero_size_mb = non_zero_size_bytes / (1024 * 1024)\n",
        "print(f\"ONNX Model Size (Non-Zero Parameters Only): {non_zero_size_mb:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA68pgYKehcS",
        "outputId": "092199ca-2ef7-47ea-f3b1-a20b9e4c20eb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Model Size (Non-Zero Parameters Only): 1.11 MB\n"
          ]
        }
      ]
    }
  ]
}